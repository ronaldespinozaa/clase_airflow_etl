{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Airflow: Conceptos Fundamentales\n",
    "\n",
    "Este notebook introduce los conceptos fundamentales de Apache Airflow para estudiantes de Data Engineering.\n",
    "\n",
    "## ¿Qué es Apache Airflow?\n",
    "\n",
    "Apache Airflow es una plataforma de código abierto para programar, monitorear y gestionar flujos de trabajo complejos. Fue desarrollado originalmente por Airbnb en 2014 y se convirtió en un proyecto de la Apache Software Foundation en 2019.\n",
    "\n",
    "### Características principales:\n",
    "\n",
    "- **Programación en Python**: Los flujos de trabajo se definen como código Python\n",
    "- **Interfaz web intuitiva**: Para monitorear y gestionar los flujos de trabajo\n",
    "- **Escalabilidad**: Puede orquestar miles de tareas\n",
    "- **Extensibilidad**: Amplio ecosistema de plugins y operadores\n",
    "- **Integración con múltiples fuentes de datos y plataformas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura de Airflow\n",
    "\n",
    "Airflow tiene una arquitectura modular compuesta por varios componentes clave:\n",
    "\n",
    "### 1. Webserver\n",
    "- Interfaz de usuario que permite visualizar DAGs, tareas, logs y métricas\n",
    "- Facilita la activación/desactivación de DAGs, retrying de tareas, etc.\n",
    "\n",
    "### 2. Scheduler\n",
    "- Componente que determina qué tareas necesitan ser ejecutadas y cuándo\n",
    "- Monitorea el estado de las tareas y DAGs\n",
    "\n",
    "### 3. Executor\n",
    "- Define cómo se ejecutan las tareas\n",
    "- Tipos comunes: LocalExecutor, CeleryExecutor, KubernetesExecutor\n",
    "\n",
    "### 4. Metastore (Base de datos)\n",
    "- Almacena metadata sobre DAGs, tareas, variables, conexiones, etc.\n",
    "- Puede ser SQLite (desarrollo), PostgreSQL, MySQL (producción)\n",
    "\n",
    "### 5. Workers\n",
    "- Procesos que realmente ejecutan las tareas (cuando se usa un ejecutor distribuido como Celery)\n",
    "\n",
    "### 6. Queue\n",
    "- Sistema de mensajería que permite la comunicación entre el Scheduler y los Workers, utilizado para distribuir tareas a los Workers en arquitecturas distribuidas(Ejemplos comunes: RabbitMQ, Redis)\n",
    "\n",
    "\n",
    "\n",
    "Airflow ejecuta DAGs en seis pasos diferentes:\n",
    "\n",
    "- El programador explora constantemente el directorio de DAGs en busca de nuevos archivos. El tiempo por defecto es cada 5 minutos.   \n",
    "- Una vez que el programador detecta un nuevo DAG, éste se procesa y se serializa en la base de datos de metadatos.   \n",
    "- El programador busca DAGs que estén listos para ejecutarse en la base de datos de metadatos. El tiempo predeterminado es cada 5 segundos.   \n",
    "- Una vez que un DAG está listo para ejecutarse, sus tareas se ponen en la cola del ejecutor.   \n",
    "- Una vez que un trabajador está disponible, recuperará una tarea de la cola para ejecutarla.   \n",
    "- El trabajador ejecutará la tarea.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptos clave en Airflow\n",
    "\n",
    "### 1. DAG (Directed Acyclic Graph)\n",
    "\n",
    "Un DAG es un grafo dirigido acíclico que representa un flujo de trabajo. Es una colección de tareas con dependencias específicas entre ellas, pero sin ciclos (una tarea no puede depender directa o indirectamente de sí misma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de definición de un DAG\n",
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "\n",
    "# Argumentos por defecto para el DAG\n",
    "default_args = {\n",
    "    'owner': 'data_engineer',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# Definición del DAG\n",
    "with DAG(\n",
    "    'ejemplo_basico',\n",
    "    default_args=default_args,\n",
    "    description='Un DAG de ejemplo',\n",
    "    schedule_interval=timedelta(days=1),\n",
    "    start_date=datetime(2023, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=['ejemplo'],\n",
    ") as dag:\n",
    "    \n",
    "    inicio = DummyOperator(task_id='inicio')\n",
    "    tarea1 = DummyOperator(task_id='tarea1')\n",
    "    tarea2 = DummyOperator(task_id='tarea2')\n",
    "    fin = DummyOperator(task_id='fin')\n",
    "    \n",
    "    # Definir dependencias entre tareas\n",
    "    inicio >> tarea1 >> fin\n",
    "    inicio >> tarea2 >> fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parámetros importantes de un DAG\n",
    "\n",
    "- **dag_id**: Identificador único del DAG\n",
    "- **description**: Descripción del propósito del DAG\n",
    "- **schedule_interval**: Frecuencia de ejecución (cron, timedelta, etc.)\n",
    "- **start_date**: Fecha desde la que se considera el DAG activo\n",
    "- **catchup**: Si debe ejecutar ejecuciones pasadas pendientes\n",
    "- **default_args**: Argumentos por defecto para todas las tareas del DAG\n",
    "- **tags**: Etiquetas para categorizar el DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Operadores\n",
    "\n",
    "Los operadores determinan qué se hace en cada tarea. Airflow proporciona muchos operadores predefinidos para tareas comunes.\n",
    "\n",
    "#### Tipos comunes de operadores:\n",
    "\n",
    "1. **BashOperator**: Ejecuta un comando bash\n",
    "2. **PythonOperator**: Ejecuta una función Python\n",
    "3. **SQLOperator**: Ejecuta consultas SQL\n",
    "4. **EmailOperator**: Envía un email\n",
    "5. **SimpleHttpOperator**: Envía una solicitud HTTP\n",
    "6. **DummyOperator**: No hace nada, útil para estructurar DAGs\n",
    "7. **Operadores específicos para servicios**: S3, GCS, BigQuery, Spark, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplos de diferentes operadores\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
    "\n",
    "# Función Python para el PythonOperator\n",
    "def procesar_datos(**kwargs):\n",
    "    print(\"Procesando datos...\")\n",
    "    return \"Datos procesados\"\n",
    "\n",
    "# En un DAG:\n",
    "tarea_bash = BashOperator(\n",
    "    task_id='ejecutar_script',\n",
    "    bash_command='echo \"Ejecutando script\" && date',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "tarea_python = PythonOperator(\n",
    "    task_id='procesar_datos',\n",
    "    python_callable=procesar_datos,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "tarea_sql = PostgresOperator(\n",
    "    task_id='crear_tabla',\n",
    "    postgres_conn_id='postgres_default',\n",
    "    sql=\"\"\"CREATE TABLE IF NOT EXISTS usuarios (id SERIAL PRIMARY KEY, nombre VARCHAR);\"\"\",\n",
    "    dag=dag,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sensores\n",
    "\n",
    "Los sensores son un tipo especial de operador que espera a que ocurra un determinado evento.\n",
    "\n",
    "#### Tipos comunes de sensores:\n",
    "\n",
    "1. **FileSensor**: Espera la aparición de un archivo\n",
    "2. **SqlSensor**: Espera que una consulta SQL devuelva resultados\n",
    "3. **HttpSensor**: Espera que un endpoint HTTP devuelva un resultado específico\n",
    "4. **ExternalTaskSensor**: Espera la finalización de una tarea en otro DAG\n",
    "5. **TimeSensor**: Espera hasta un momento específico del día"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de un sensor\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "\n",
    "# En un DAG:\n",
    "esperar_archivo = FileSensor(\n",
    "    task_id='esperar_archivo_csv',\n",
    "    filepath='/data/eventos.csv',\n",
    "    poke_interval=300,  # Verificar cada 5 minutos\n",
    "    timeout=60 * 60 * 24,  # Timeout después de 24 horas\n",
    "    mode='poke',  # Modo de sensado\n",
    "    dag=dag,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tasks\n",
    "\n",
    "Una tarea es una instancia de un operador y representa un trabajo específico que debe realizarse como parte de un DAG.\n",
    "\n",
    "#### Ciclo de vida de una tarea:\n",
    "\n",
    "1. **none**: Estado inicial\n",
    "2. **scheduled**: Programada para ejecución\n",
    "3. **queued**: En cola para ejecución\n",
    "4. **running**: En ejecución\n",
    "5. **success/failed/skipped/upstream_failed**: Estados finales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Task dependencies\n",
    "\n",
    "Las dependencias entre tareas determinan el orden de ejecución. En Airflow, estas se pueden definir usando diferentes sintaxis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diferentes formas de definir dependencias entre tareas\n",
    "\n",
    "# Método 1: Operador de bit shift\n",
    "tarea_a >> tarea_b >> tarea_c  # A se ejecuta antes que B, B antes que C\n",
    "tarea_a << tarea_b << tarea_c  # C se ejecuta antes que B, B antes que A\n",
    "\n",
    "# Método 2: Método set_upstream/set_downstream\n",
    "tarea_b.set_upstream(tarea_a)   # A se ejecuta antes que B\n",
    "tarea_b.set_downstream(tarea_c) # C se ejecuta después que B\n",
    "\n",
    "# Dependencias complejas\n",
    "tarea_inicio >> [tarea_a, tarea_b, tarea_c] >> tarea_union  # Bifurcación y unión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. XComs (Cross-Communication)\n",
    "\n",
    "XComs (Cross-Communications) permite que las tareas intercambien pequeñas cantidades de datos entre sí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso de XComs\n",
    "def tarea_emisora(**kwargs):\n",
    "    # Guardar un valor en XCom\n",
    "    kwargs['ti'].xcom_push(key='resultado_analisis', value={'estado': 'completado', 'registros': 1250})\n",
    "    return \"Tarea completada\"\n",
    "\n",
    "def tarea_receptora(**kwargs):\n",
    "    # Recuperar valor de XCom\n",
    "    ti = kwargs['ti']\n",
    "    resultado = ti.xcom_pull(task_ids='tarea_emisora', key='resultado_analisis')\n",
    "    print(f\"Resultado recuperado: {resultado}\")\n",
    "    return f\"Procesados {resultado['registros']} registros\"\n",
    "\n",
    "# En un DAG:\n",
    "emisor = PythonOperator(\n",
    "    task_id='tarea_emisora',\n",
    "    python_callable=tarea_emisora,\n",
    "    provide_context=True,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "receptor = PythonOperator(\n",
    "    task_id='tarea_receptora',\n",
    "    python_callable=tarea_receptora,\n",
    "    provide_context=True,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "emisor >> receptor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Variables\n",
    "\n",
    "Las variables de Airflow permiten almacenar y recuperar valores que se utilizan en los DAGs. Se almacenan en la base de datos de Airflow y se pueden acceder a través del código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso de Variables de Airflow\n",
    "from airflow.models import Variable\n",
    "\n",
    "# En un operador Python:\n",
    "def procesar_con_config(**kwargs):\n",
    "    # Recuperar una variable\n",
    "    api_key = Variable.get(\"api_key\")\n",
    "    entorno = Variable.get(\"entorno\", default_var=\"desarrollo\")\n",
    "    \n",
    "    # Variables pueden ser JSON\n",
    "    config = Variable.get(\"config_json\", deserialize_json=True)\n",
    "    umbral = config.get('umbral', 100)\n",
    "    \n",
    "    print(f\"Procesando en entorno {entorno} con umbral {umbral}\")\n",
    "    return \"Procesamiento completo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Conexiones\n",
    "\n",
    "Las conexiones almacenan información para conectarse a sistemas externos (bases de datos, APIs, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso de una conexión en un operador\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "\n",
    "def consultar_base_datos(**kwargs):\n",
    "    # Crear un hook usando el ID de conexión\n",
    "    postgres_hook = PostgresHook(postgres_conn_id='postgres_data_warehouse')\n",
    "    \n",
    "    # Ejecutar una consulta\n",
    "    registros = postgres_hook.get_records(\"SELECT * FROM ventas LIMIT 10\")\n",
    "    \n",
    "    print(f\"Se encontraron {len(registros)} registros\")\n",
    "    return registros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptos avanzados\n",
    "\n",
    "### 1. Branching\n",
    "\n",
    "El branching permite tomar diferentes caminos en un DAG basado en alguna lógica o condición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de branching\n",
    "from airflow.operators.python import BranchPythonOperator\n",
    "\n",
    "def elegir_camino(**kwargs):\n",
    "    # Alguna lógica para decidir qué camino tomar\n",
    "    if Variable.get(\"entorno\") == \"produccion\":\n",
    "        return 'tarea_produccion'\n",
    "    else:\n",
    "        return 'tarea_desarrollo'\n",
    "\n",
    "# En un DAG:\n",
    "rama = BranchPythonOperator(\n",
    "    task_id='elegir_camino',\n",
    "    python_callable=elegir_camino,\n",
    "    provide_context=True,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "tarea_prod = DummyOperator(task_id='tarea_produccion', dag=dag)\n",
    "tarea_dev = DummyOperator(task_id='tarea_desarrollo', dag=dag)\n",
    "final = DummyOperator(task_id='tarea_final', trigger_rule='one_success', dag=dag)\n",
    "\n",
    "rama >> [tarea_prod, tarea_dev] >> final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Trigger Rules\n",
    "\n",
    "Las reglas de disparo determinan cuándo una tarea debe ejecutarse basado en el estado de sus tareas upstream.\n",
    "\n",
    "Opciones comunes:\n",
    "\n",
    "- **all_success**: Todas las tareas upstream deben tener éxito (default)\n",
    "- **all_failed**: Todas las tareas upstream deben fallar\n",
    "- **all_done**: Todas las tareas upstream deben completarse (éxito o fallo)\n",
    "- **one_success**: Al menos una tarea upstream debe tener éxito\n",
    "- **one_failed**: Al menos una tarea upstream debe fallar\n",
    "- **none_failed**: Todas las tareas upstream no deben fallar (pueden tener éxito o ser omitidas)\n",
    "- **none_skipped**: Ninguna tarea upstream debe ser omitida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de trigger rule\n",
    "tarea_final = DummyOperator(\n",
    "    task_id='tarea_final',\n",
    "    trigger_rule='one_success',  # Se ejecuta si al menos una tarea upstream tiene éxito\n",
    "    dag=dag,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SubDAGs\n",
    "\n",
    "Los SubDAGs permiten encapsular un conjunto de tareas como una sola tarea dentro de un DAG principal, lo que ayuda a organizar flujos de trabajo complejos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de SubDAG\n",
    "from airflow.operators.subdag import SubDagOperator\n",
    "\n",
    "# Función que crea y devuelve un subDAG\n",
    "def crear_subdag_procesamiento(parent_dag_id, child_dag_id, args):\n",
    "    with DAG(\n",
    "        dag_id=f'{parent_dag_id}.{child_dag_id}',\n",
    "        default_args=args,\n",
    "        schedule_interval=None,\n",
    "    ) as dag_hijo:\n",
    "        tarea1 = DummyOperator(task_id='subtarea1', dag=dag_hijo)\n",
    "        tarea2 = DummyOperator(task_id='subtarea2', dag=dag_hijo)\n",
    "        tarea1 >> tarea2\n",
    "        return dag_hijo\n",
    "\n",
    "# En el DAG principal:\n",
    "procesamiento = SubDagOperator(\n",
    "    task_id='procesar_datos',\n",
    "    subdag=crear_subdag_procesamiento('dag_principal', 'procesar_datos', default_args),\n",
    "    dag=dag_principal,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. TaskGroups\n",
    "\n",
    "TaskGroups es una alternativa más moderna a SubDAGs, y proporciona una manera de agrupar visualmente las tareas en la interfaz de usuario sin las complejidades de los SubDAGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de TaskGroup\n",
    "from airflow.utils.task_group import TaskGroup\n",
    "\n",
    "# En un DAG:\n",
    "with TaskGroup(group_id='procesar_datos') as grupo_procesamiento:\n",
    "    tarea1 = DummyOperator(task_id='extraer')\n",
    "    tarea2 = DummyOperator(task_id='transformar')\n",
    "    tarea3 = DummyOperator(task_id='cargar')\n",
    "    \n",
    "    tarea1 >> tarea2 >> tarea3\n",
    "\n",
    "inicio >> grupo_procesamiento >> fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Pools\n",
    "\n",
    "Los pools limitan el número de tareas que pueden ejecutarse simultáneamente en un grupo específico, lo que permite controlar el uso de recursos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso de pool\n",
    "tarea_intensiva = BashOperator(\n",
    "    task_id='tarea_intensiva_cpu',\n",
    "    bash_command='echo \"Procesando datos\" && sleep 30',\n",
    "    pool='cpu_intensive',  # Asignar a un pool específico\n",
    "    pool_slots=2,          # Usar 2 slots del pool\n",
    "    dag=dag,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejores prácticas en Airflow\n",
    "\n",
    "### 1. Idempotencia\n",
    "\n",
    "Las tareas deben ser idempotentes, es decir, deben producir el mismo resultado independientemente de cuántas veces se ejecuten con los mismos parámetros.\n",
    "\n",
    "```python\n",
    "# Malo: Insertar directamente sin verificar\n",
    "INSERT INTO tabla VALUES (1, 'valor');\n",
    "\n",
    "# Bueno: Usar INSERT IGNORE o ON CONFLICT\n",
    "INSERT INTO tabla VALUES (1, 'valor') ON CONFLICT (id) DO NOTHING;\n",
    "```\n",
    "\n",
    "### 2. Atomicidad\n",
    "\n",
    "Cada tarea debe hacer una sola cosa bien definida. Evita tareas que hagan demasiadas cosas diferentes.\n",
    "\n",
    "```python\n",
    "# Malo: Una sola tarea que hace todo\n",
    "def extraer_transformar_cargar():\n",
    "    datos = extraer()\n",
    "    datos_transformados = transformar(datos)\n",
    "    cargar(datos_transformados)\n",
    "\n",
    "# Bueno: Dividir en tareas atómicas\n",
    "def extraer():\n",
    "    return obtener_datos()\n",
    "\n",
    "def transformar(**kwargs):\n",
    "    ti = kwargs['ti']\n",
    "    datos = ti.xcom_pull(task_ids='extraer')\n",
    "    return procesar_datos(datos)\n",
    "\n",
    "def cargar(**kwargs):\n",
    "    ti = kwargs['ti']\n",
    "    datos_transformados = ti.xcom_pull(task_ids='transformar')\n",
    "    guardar_en_bd(datos_transformados)\n",
    "```\n",
    "\n",
    "### 3. Evitar dependencias innecesarias entre DAGs\n",
    "\n",
    "Minimiza el acoplamiento entre DAGs diferentes para facilitar el mantenimiento y la escalabilidad.\n",
    "\n",
    "### 4. Manejo adecuado de errores\n",
    "\n",
    "Implementa manejo de errores y estrategias de retry apropiadas para cada tarea.\n",
    "\n",
    "```python\n",
    "tarea = PythonOperator(\n",
    "    task_id='tarea_con_retry',\n",
    "    python_callable=funcion_propensa_a_fallos,\n",
    "    retries=3,\n",
    "    retry_delay=timedelta(minutes=5),\n",
    "    retry_exponential_backoff=True,\n",
    "    max_retry_delay=timedelta(hours=1),\n",
    "    dag=dag,\n",
    ")\n",
    "```\n",
    "\n",
    "### 5. No usar datos modificables entre tareas\n",
    "\n",
    "Evita compartir datos modificables (como listas o diccionarios globales) entre tareas, ya que puede llevar a comportamientos inesperados."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
